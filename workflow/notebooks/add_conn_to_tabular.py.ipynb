{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bebca3a-2c12-44d2-a3f4-b81d1e4a7f53",
   "metadata": {},
   "source": [
    "# SFC, FC & SC - LOBE vs HCP #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009e1304-63e0-441b-a9de-0ec57849577d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snakemake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subjects\u001b[38;5;241m=\u001b[39m\u001b[43msnakemake\u001b[49m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39msubjects\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#for dataset in subjects.keys():\u001b[39;00m\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;66;03m#   subjects[dataset] = [ 'sub-'+subject for subject in subjects[dataset] ]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snakemake' is not defined"
     ]
    }
   ],
   "source": [
    "subjects=snakemake.params.subjects\n",
    "\n",
    "#for dataset in subjects.keys():\n",
    " #   subjects[dataset] = [ 'sub-'+subject for subject in subjects[dataset] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91165d2-4d7f-4c62-ab77-c1a073ade18a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subjects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubjects\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subjects' is not defined"
     ]
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675130f2-8089-4dc5-8a7f-dda9c9c565f3",
   "metadata": {},
   "source": [
    "## Importing packages and setting paths ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feb603c4-b73e-450c-9786-3eceb06ed625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c02177-09c0-499d-99e8-4c9e5d5ebe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"snakemake.params.subjects[{dataset}] = {snakemake.params.subjects[dataset]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea528f30-1a42-4d1d-92c8-404aae934a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resources/atlas/atlas-brainnetome_dseg.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#first load up the label dseg table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m atlas\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrainnetome\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresources/atlas/atlas-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43matlas\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_dseg.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_labels\n\u001b[1;32m      7\u001b[0m df_network_names \u001b[38;5;241m=\u001b[39m df_labels\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m/local/scratch/.cache/pypoetry/virtualenvs/lobe-analysis-FslWevsl-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/scratch/.cache/pypoetry/virtualenvs/lobe-analysis-FslWevsl-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/local/scratch/.cache/pypoetry/virtualenvs/lobe-analysis-FslWevsl-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/scratch/.cache/pypoetry/virtualenvs/lobe-analysis-FslWevsl-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/local/scratch/.cache/pypoetry/virtualenvs/lobe-analysis-FslWevsl-py3.11/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/atlas/atlas-brainnetome_dseg.tsv'"
     ]
    }
   ],
   "source": [
    "#first load up the label dseg table\n",
    "atlas='brainnetome'\n",
    "\n",
    "df_labels = pd.read_csv(f'resources/atlas/atlas-{atlas}_dseg.tsv',sep='\\t')\n",
    "df_labels\n",
    "\n",
    "df_network_names = df_labels.networks.unique()\n",
    "df_network_names\n",
    "\n",
    "df_tabular = pd.read_csv(snakemake.input.tsv,sep='\\t')\n",
    "#df_tabular = pd.read_csv('results/analysis/df_tabular.csv',sep='\\t')\n",
    "\n",
    "df_tabular\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43561a46-47b4-4450-a04f-9826b66ecf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schaefer Patterns\n",
    "#fc_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-schaefer_bold.pconn.nii'\n",
    "#sc_pattern='results/{dataset}/sub-{subject}/dwi/sub-{subject}_den-91k_atlas-schaefer_struc.pconn.nii'\n",
    "#sfc_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-schaefer_sfc.pscalar.nii'\n",
    "\n",
    "# Brainnetome Patterns\n",
    "# Schaefer Patterns\n",
    "fc_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-brainnetome_bold.pconn.nii'\n",
    "sc_pattern='results/{dataset}/sub-{subject}/dwi/sub-{subject}_den-91k_atlas-brainnetome_struc.pconn.nii'\n",
    "sfc_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-brainnetome_sfc.pscalar.nii'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2eb9b7-a219-4370-853c-f683277b6e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfc_paths={ dataset: sorted([sfc_pattern.format(dataset=dataset,subject=subject) for subject in subjects[dataset]]) \n",
    "               for dataset in subjects.keys() }\n",
    "\n",
    "fc_paths={ dataset: sorted([fc_pattern.format(dataset=dataset,subject=subject) for subject in subjects[dataset]]) \n",
    "               for dataset in subjects.keys() }\n",
    "\n",
    "sc_paths={ dataset: sorted([sc_pattern.format(dataset=dataset,subject=subject) for subject in subjects[dataset]]) \n",
    "               for dataset in subjects.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1e28751-82aa-42f3-b2e2-649b23f7ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in sfc_paths['LOBE']: 0\n"
     ]
    }
   ],
   "source": [
    "#from snakebids.utils.snakemake_io import glob_wildcards\n",
    "#subjects = glob_wildcards(sfc_pattern.format(dataset='HCP',subject='{subject}'))\n",
    "\n",
    "if 'LOBE' in sfc_paths:\n",
    "    print(f\"Number of files in sfc_paths['LOBE']: {len(sfc_paths['LOBE'])}\")\n",
    "else:\n",
    "    print(\"'LOBE' key is not in sfc_paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2e154a-d4c4-473e-858f-33d3af6b0d2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m fc\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m      3\u001b[0m sc\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 4\u001b[0m num_parcels \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\u001b[43msfc_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLOBE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mget_fdata()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m datasets\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHCP\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOBE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sfc={}\n",
    "fc={}\n",
    "sc={}\n",
    "num_parcels = nib.load(sfc_paths['LOBE'][0]).get_fdata().shape[1]\n",
    "datasets=['HCP','LOBE']\n",
    "for dataset in datasets:\n",
    "    sfc[dataset] = np.concatenate(  [ nib.load(path).get_fdata()  for path in sfc_paths[dataset]] ,axis=0)\n",
    "    fc[dataset] = np.concatenate(  [ nib.load(path).get_fdata().reshape((1,num_parcels,num_parcels))  for path in fc_paths[dataset]] ,axis=0)\n",
    "    sc[dataset] = np.concatenate(  [ nib.load(path).get_fdata().reshape((1,num_parcels,num_parcels))  for path in sc_paths[dataset]] ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe5e34-e2e1-4473-a72f-a1f8d9069f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac46bd71-4c51-4f42-a55b-27d1d7266d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     participant_id dataset        age sex  age_seizure_onset  \\\n",
      "0    sub-HCD0001305     HCP  11.916667   M                NaN   \n",
      "1    sub-HCD0021614     HCP   9.166667   F                NaN   \n",
      "2    sub-HCD0026119     HCP  15.166667   F                NaN   \n",
      "3    sub-HCD0041822     HCP  17.416667   M                NaN   \n",
      "4    sub-HCD0042420     HCP  18.333333   M                NaN   \n",
      "..              ...     ...        ...  ..                ...   \n",
      "357         sub-029    LOBE  16.000000   M               15.0   \n",
      "358         sub-031    LOBE   9.000000   M                4.0   \n",
      "359         sub-033    LOBE   7.000000   M                4.0   \n",
      "360         sub-035    LOBE   7.000000   M                2.0   \n",
      "361         sub-038    LOBE  10.000000   F                7.0   \n",
      "\n",
      "     age_ep_diagnosis  ep_duration_first_scan  seizure_duration_first_scan  \\\n",
      "0                 NaN                     NaN                          NaN   \n",
      "1                 NaN                     NaN                          NaN   \n",
      "2                 NaN                     NaN                          NaN   \n",
      "3                 NaN                     NaN                          NaN   \n",
      "4                 NaN                     NaN                          NaN   \n",
      "..                ...                     ...                          ...   \n",
      "357              15.0                     1.0                          1.0   \n",
      "358               6.0                     3.0                          5.0   \n",
      "359               NaN                     NaN                          3.0   \n",
      "360               4.0                     3.0                          5.0   \n",
      "361               7.0                     3.0                          3.0   \n",
      "\n",
      "     pnes  benign_rolandic  ... NFC_Limbic  NFC_Cont  NFC_Default   NSC_Vis  \\\n",
      "0     NaN              NaN  ...   0.200019  0.257366     0.135539  3.885010   \n",
      "1     NaN              NaN  ...  -0.000347  0.147511     0.039930  4.216869   \n",
      "2     NaN              NaN  ...   0.093052  0.082952     0.052142  4.770188   \n",
      "3     NaN              NaN  ...  -0.001980 -0.008243     0.010302  4.030981   \n",
      "4     NaN              NaN  ...   0.091706  0.153798     0.082643  4.350406   \n",
      "..    ...              ...  ...        ...       ...          ...       ...   \n",
      "357   0.0              0.0  ...   0.467955  0.392260     0.372756  4.093281   \n",
      "358   0.0              0.0  ...   0.228163  0.205326     0.162994  4.788529   \n",
      "359   0.0              0.0  ...  -0.029804  0.006911    -0.027776  4.030132   \n",
      "360   0.0              0.0  ...   0.035206 -0.010172     0.008450  4.027565   \n",
      "361   0.0              0.0  ...   0.092600  0.031936     0.105244  4.227500   \n",
      "\n",
      "     NSC_SomMot  NSC_DorsAttn  NSC_SalVentAttn  NSC_Limbic  NSC_Cont  \\\n",
      "0      5.062987      8.027959         7.580041    5.651460  7.296828   \n",
      "1      4.518716      7.629381         7.669835    6.542762  7.380725   \n",
      "2      4.651514      8.898358         7.785734    6.827971  7.798882   \n",
      "3      4.773803      8.804379         7.742067    6.960907  8.298339   \n",
      "4      4.851836      9.014367         7.819415    6.471102  8.094806   \n",
      "..          ...           ...              ...         ...       ...   \n",
      "357    4.358512      7.515517         7.141540    6.457969  6.372687   \n",
      "358    5.042807      8.379897         7.876680    7.567836  8.409854   \n",
      "359    4.791794      7.306059         7.469537    6.275113  7.390206   \n",
      "360    4.504583      7.768704         7.925532    6.047853  7.735365   \n",
      "361    5.029610      8.060599         7.408326    5.981137  7.941198   \n",
      "\n",
      "     NSC_Default  \n",
      "0       7.935002  \n",
      "1       8.132475  \n",
      "2       8.109647  \n",
      "3       8.540095  \n",
      "4       8.193476  \n",
      "..           ...  \n",
      "357     7.238880  \n",
      "358     8.806705  \n",
      "359     8.429542  \n",
      "360     8.119764  \n",
      "361     8.547162  \n",
      "\n",
      "[362 rows x 978 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae272858-9fe4-4d73-b18a-55f7fc58bd7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snakemake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m df_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(sfc[dataset],\n\u001b[1;32m      6\u001b[0m              columns\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRSFC_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdf_labels\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m      8\u001b[0m df_tabular_ \u001b[38;5;241m=\u001b[39m df_tabular\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msnakemake\u001b[49m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39msubjects[dataset]\n\u001b[1;32m     11\u001b[0m df_merged_ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_tabular_,df_,how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m,on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m df_list\u001b[38;5;241m.\u001b[39mappend(df_merged_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snakemake' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#this adds RSFC features as new columns to the df_tabular\n",
    "\n",
    "df_list=[]\n",
    "for dataset in datasets:\n",
    "    df_=pd.DataFrame(sfc[dataset],\n",
    "                 columns=('RSFC_'+df_labels.name))\n",
    "    \n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    #df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "\n",
    "    # Check that the number of rows in df_ matches the number of subjects\n",
    "    participant_ids = snakemake.params.subjects[dataset]\n",
    "    \n",
    "    # Assign participant IDs directly to df_\n",
    "    df_['participant_id'] = participant_ids\n",
    "    \n",
    "    df_merged_ = pd.merge(df_tabular_,df_,how='left',on='participant_id')\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "df_tabular = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a03452-b106-4b75-a9c9-cd3540a32153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of sfc[dataset] for 'LOBE': {len(sfc[dataset])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde68925-3d3a-42c7-9f44-e4995175e04e",
   "metadata": {},
   "source": [
    "## Average FC per parcel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "505fbb53-cc82-4ea1-a28d-e9a8b75235dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set diagonal to be nan\n",
    "x1 = fc['HCP']\n",
    "for s in range(x1.shape[0]):\n",
    "    conn = x1[s,:,:]    \n",
    "    mask = np.eye(num_parcels)\n",
    "    conn[mask==1] = np.nan\n",
    "    x1[s,:,:] = conn\n",
    "\n",
    "x2 = fc['LOBE']\n",
    "for s in range(x2.shape[0]):\n",
    "    conn = x2[s,:,:]\n",
    "    mask = np.eye(num_parcels)\n",
    "    conn[mask==1] = np.nan\n",
    "    x2[s,:,:] = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d6e7c7f-93fa-4675-b0f8-ae092f3ab3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 246, 246)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de4b67dd-8164-4ebe-a0fa-76203f270d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1428816/3377732870.py:2: RuntimeWarning: Mean of empty slice\n",
      "  x2 = np.nanmean(x2,axis=2)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.nanmean(x1,axis=2)\n",
    "x2 = np.nanmean(x2,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1cd7545-5740-4021-ac4e-75838de95e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 246)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad83ddfe-7d28-4805-81c3-52c9927154ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 246)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77612122-7ab3-48ba-bb7b-fadc1ed013d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add FC parcel avgs to tabular df\n",
    "\n",
    "data_={'HCP':x1,'LOBE':x2}\n",
    "\n",
    "df_list=[]\n",
    "for dataset in datasets:\n",
    "\n",
    "    \n",
    "    df_=pd.DataFrame(data_[dataset],\n",
    "                 columns=('RFC_'+df_labels.name))\n",
    "    \n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "\n",
    "    \n",
    "    df_merged_ = pd.merge(df_tabular_,df_,how='left',on='participant_id')\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "df_tabular = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c1aac-5478-4bb0-94d5-8b80ba390a19",
   "metadata": {},
   "source": [
    "# Structural Connectivity #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cda9e7-05f9-40f1-aacd-bcb5a104a27d",
   "metadata": {},
   "source": [
    "## Average SC per parcel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f31ff5ca-6866-48d7-b076-c2803b05de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set diagonal to be nan\n",
    "x1 = sc['HCP']\n",
    "for s in range(x1.shape[0]):\n",
    "    conn = x1[s,:,:]    \n",
    "    mask = np.eye(num_parcels)\n",
    "    conn[mask==1] = np.nan\n",
    "    x1[s,:,:] = conn\n",
    "\n",
    "x2 = sc['LOBE']\n",
    "for s in range(x2.shape[0]):\n",
    "    conn = x2[s,:,:]\n",
    "    mask = np.eye(num_parcels)\n",
    "    conn[mask==1] = np.nan\n",
    "    x2[s,:,:] = conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a57c1723-668c-4e39-b942-595302190e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 246, 246)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8808731-0243-4c28-9b18-3b75a0685b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 246)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.nanmean(x1,axis=2)\n",
    "x1.shape\n",
    "x2 = np.nanmean(x2,axis=2)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "407e956b-7d84-4ed7-9d39-a586a45ec619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 246)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "109c3923-2baf-418e-8c56-737525cfc1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 246)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409b33bf-d980-4cb3-96ba-19bf34d7be81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#add SC parcel avgs to tabular df\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data_\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHCP\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mx1\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOBE\u001b[39m\u001b[38;5;124m'\u001b[39m:x2}\n\u001b[1;32m      5\u001b[0m df_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "#add SC parcel avgs to tabular df\n",
    "\n",
    "data_={'HCP':x1,'LOBE':x2}\n",
    "\n",
    "df_list=[]\n",
    "for dataset in datasets:\n",
    "\n",
    "    \n",
    "    df_=pd.DataFrame(data_[dataset],\n",
    "                 columns=('RSC_'+df_labels.name))\n",
    "    \n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "\n",
    "    df_merged_ = pd.merge(df_tabular_,df_,how='left',on='participant_id')\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "df_tabular = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422664a0-bb44-4a0c-9a06-5a4ea9f653cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444036b-9dcc-486b-9f6d-476ebd4c552e",
   "metadata": {},
   "source": [
    "# Network-Wise, LOBE vs HCP #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564b2d-25ee-4b07-ae9d-568b0c2a3700",
   "metadata": {},
   "source": [
    "## Paths ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61be1321-36c3-425f-9e7a-2d740df73d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brainnetome Patterns\n",
    "fc_network_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-brainnetome_netbold.pconn.nii'\n",
    "sc_network_pattern='results/{dataset}/sub-{subject}/dwi/sub-{subject}_den-91k_atlas-brainnetome_netstruc.pconn.nii'\n",
    "sfc_network_pattern='results/{dataset}/sub-{subject}/func/sub-{subject}_task-rest_den-91k_desc-preproc_denoise-24HMP8PhysSpikeReg_fwhm-5_atlas-brainnetome_netsfc.pconn.nii'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1061c13-0457-4357-85d6-ae675909a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfc_netpaths={}\n",
    "sfc_netpaths['HCP'] = sorted(glob(sfc_network_pattern.format(dataset='HCP',subject='*')))\n",
    "sfc_netpaths['LOBE'] = sorted(glob(sfc_network_pattern.format(dataset='LOBE',subject='*')))\n",
    "\n",
    "fc_netpaths={}\n",
    "fc_netpaths['HCP'] = sorted(glob(fc_network_pattern.format(dataset='HCP',subject='*')))\n",
    "fc_netpaths['LOBE'] = sorted(glob(fc_network_pattern.format(dataset='LOBE',subject='*')))\n",
    "\n",
    "sc_netpaths={}\n",
    "sc_netpaths['HCP'] = sorted(glob(sc_network_pattern.format(dataset='HCP',subject='*')))\n",
    "sc_netpaths['LOBE'] = sorted(glob(sc_network_pattern.format(dataset='LOBE',subject='*')))\n",
    "\n",
    "#print(sfc_netpaths)\n",
    "#print(fc_netpaths)\n",
    "#print(sc_netpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5a02458-6527-48f9-9a7b-33d774f80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfc={}\n",
    "fc={}\n",
    "sc={}\n",
    "num_parcels = nib.load(sfc_netpaths[dataset][0]).get_fdata().shape[1]\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    sfc[dataset] = np.concatenate(  [ nib.load(path).get_fdata().reshape((1,num_parcels,num_parcels))  for path in sfc_netpaths[dataset]] ,axis=0)  \n",
    "    #for path in sfc_netpaths[dataset]] ,axis=0)\n",
    "    fc[dataset] = np.concatenate(  [ nib.load(path).get_fdata().reshape((1,num_parcels,num_parcels))  for path in fc_netpaths[dataset]] ,axis=0)\n",
    "    sc[dataset] = np.concatenate(  [ nib.load(path).get_fdata().reshape((1,num_parcels,num_parcels))  for path in sc_netpaths[dataset]] ,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88018d32-86e2-46df-b867-0894b0a640a1",
   "metadata": {},
   "source": [
    "# Network SFC #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "41e644c3-cdac-484b-8deb-3f2a8b739a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_={}\n",
    "for dataset in ['HCP','LOBE']:\n",
    "\n",
    "    x1 = sfc[dataset]\n",
    "    data_[dataset] = np.zeros((x1.shape[0],int((num_parcels*(num_parcels+1))/2)))\n",
    "    num_parcels = x1.shape[1]  # Assuming it's 8\n",
    "\n",
    "    # Loop over each \"slice\" in x2 (assuming only one slice, so loop over the first element)\n",
    "    for s in range(x1.shape[0]):\n",
    "        conn = x1[s, :, :]  # Extract the 8x8 matrix for the current subject        \n",
    "        # Now loop over the upper triangle to gather values for each \"mixture\"\n",
    "        values_subj=[]\n",
    "        for i in range(num_parcels):\n",
    "            for j in range(i, num_parcels): # Include diagonal as well by starting at i\n",
    "               values_subj.append(conn[i, j])  # Get the value for that specific pair (i, j)\n",
    "    \n",
    "        data_[dataset][s,:] = values_subj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e34cf8-4b4f-4772-ac13-adefe6039c32",
   "metadata": {},
   "source": [
    "## Network SFC to DF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b96384-a686-4a6a-a505-beb90303cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    colnames=[]\n",
    "    for i in range(len(df_network_names)):\n",
    "        for j in range(i, len(df_network_names)):\n",
    "\n",
    "            colnames.append(f'NSFC_{df_network_names[i]}_{df_network_names[j]}')\n",
    "                \n",
    "\n",
    "    df_ = pd.DataFrame(data_[dataset], columns=colnames)\n",
    "    \n",
    "    # Filter the main df_tabular by the current dataset\n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    \n",
    "    # Add the participant_ids from the subjects\n",
    "    #df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "    df_['participant_id'] = ['sub-'+subject for subject in subjects[dataset]]\n",
    "    \n",
    "    # Merge the new df_ with df_tabular_ on 'participant_id'\n",
    "    df_merged_ = pd.merge(df_tabular_, df_, how='left', on='participant_id')\n",
    "    \n",
    "    # Append merged DataFrame to the list\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "# Concatenate all merged DataFrames\n",
    "df_tabular = pd.concat(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "406b463f-e832-4d14-a2c9-eb495da8aaaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snakemake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m df_tabular_ \u001b[38;5;241m=\u001b[39m df_tabular\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Add the participant_ids from the subjects\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m df_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msnakemake\u001b[49m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39msubjects[dataset]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#df_['participant_id'] = subjects[dataset]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Merge the new df_ with df_tabular_ on 'participant_id'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m df_merged_ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_tabular_, df_, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snakemake' is not defined"
     ]
    }
   ],
   "source": [
    "# # delete if above works\n",
    "# data_={'HCP':x1,'LOBE':x2}  \n",
    "\n",
    "# # Example datasets\n",
    "# datasets = ['LOBE', 'HCP']\n",
    "\n",
    "# # Example: x2 is a (1, 8, 8) matrix\n",
    "# #changed\n",
    "# #x2 = x2[0]  # Now x2 is shape (8, 8)\n",
    "\n",
    "# # Assuming df_tabular is already loaded\n",
    "# # df_tabular = pd.read_csv(\"your_file.csv\")  # Or however you load df_tabular\n",
    "\n",
    "# # List to collect merged DataFrames\n",
    "# df_list = []\n",
    "\n",
    "# for dataset in datasets:\n",
    "#     # Create column names: NSFC_network1_network2\n",
    "#     colnames=[]\n",
    "#     for i in range(len(df_network_names)):\n",
    "#         for j in range(i, len(df_network_names)):\n",
    "#             if i>j:\n",
    "#                 continue\n",
    "#             colnames.append(f'NSFC_{df_network_names[i]}_{df_network_names[j]}')\n",
    "                \n",
    "       \n",
    "\n",
    "#     # Flatten the upper triangle of x2 into a 1D array to match the number of columns\n",
    "#     upper_triangle_values = [x2[i, j] for i in range(len(df_network_names)) for j in range(i, len(df_network_names))]\n",
    "\n",
    "#     # Create a DataFrame for the current dataset using the flattened upper triangle values\n",
    "#     df_ = pd.DataFrame([upper_triangle_values], columns=colnames)\n",
    "    \n",
    "#     # Filter the main df_tabular by the current dataset\n",
    "#     df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    \n",
    "#     # Add the participant_ids from the subjects\n",
    "#     df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "#     #df_['participant_id'] = subjects[dataset]\n",
    "    \n",
    "#     # Merge the new df_ with df_tabular_ on 'participant_id'\n",
    "#     df_merged_ = pd.merge(df_tabular_, df_, how='left', on='participant_id')\n",
    "    \n",
    "#     # Append merged DataFrame to the list\n",
    "#     df_list.append(df_merged_)\n",
    "\n",
    "# # Concatenate all merged DataFrames\n",
    "# df_tabular_final = pd.concat(df_list)\n",
    "\n",
    "# # Display the final DataFrame\n",
    "# df_tabular_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecc160-53c6-4396-9a1f-b3fb86d00f94",
   "metadata": {},
   "source": [
    "# Network FC #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48e469-9cd9-42c6-89df-37f0229096a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_={}\n",
    "for dataset in ['HCP','LOBE']:\n",
    "\n",
    "    x1 = fc[dataset]\n",
    "    data_[dataset] = np.zeros((x1.shape[0],int((num_parcels*(num_parcels+1))/2)))\n",
    "    num_parcels = x1.shape[1]  # Assuming it's 8\n",
    "\n",
    "    for s in range(x1.shape[0]):\n",
    "        conn = x1[s, :, :]  # Extract the 8x8 matrix for the current subject        \n",
    "        # Now loop over the upper triangle to gather values for each \"mixture\"\n",
    "        values_subj=[]\n",
    "        for i in range(num_parcels):\n",
    "            for j in range(i, num_parcels): # Include diagonal as well by starting at i\n",
    "\n",
    "               values_subj.append(conn[i, j])  # Get the value for that specific pair (i, j)\n",
    "    \n",
    "        data_[dataset][s,:] = values_subj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42362c9a-5668-41b5-855c-18405333bb8a",
   "metadata": {},
   "source": [
    "## Network FC to DF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5d94f-e76d-4317-9d1f-a2cb3f9a0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    colnames=[]\n",
    "    for i in range(len(df_network_names)):\n",
    "        for j in range(i, len(df_network_names)):\n",
    "\n",
    "            colnames.append(f'NFC_{df_network_names[i]}_{df_network_names[j]}')\n",
    "                \n",
    "    df_ = pd.DataFrame(data_[dataset], columns=colnames)\n",
    "    \n",
    "    # Filter the main df_tabular by the current dataset\n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    \n",
    "    # Add the participant_ids from the subjects\n",
    "    #df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "    df_['participant_id'] = ['sub-'+subject for subject in subjects[dataset]]\n",
    "    \n",
    "    # Merge the new df_ with df_tabular_ on 'participant_id'\n",
    "    df_merged_ = pd.merge(df_tabular_, df_, how='left', on='participant_id')\n",
    "    \n",
    "    # Append merged DataFrame to the list\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "# Concatenate all merged DataFrames\n",
    "df_tabular = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48f2a0-5237-4e00-9295-48cbd881d756",
   "metadata": {},
   "source": [
    "# Network SC #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7ed9b-a224-4101-bb0b-dee068a19f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_={}\n",
    "for dataset in ['HCP','LOBE']:\n",
    "\n",
    "    x1 = sc[dataset]\n",
    "    data_[dataset] = np.zeros((x1.shape[0],int((num_parcels*(num_parcels+1))/2)))\n",
    "    num_parcels = x1.shape[1]  # Assuming it's 8\n",
    "\n",
    "    # Loop over each \"slice\" in x2 (assuming only one slice, so loop over the first element)\n",
    "    for s in range(x1.shape[0]):\n",
    "        conn = x1[s, :, :]  # Extract the 8x8 matrix for the current subject        \n",
    "        # Now loop over the upper triangle to gather values for each \"mixture\"\n",
    "        values_subj=[]\n",
    "        for i in range(num_parcels):\n",
    "            for j in range(i, num_parcels): # Include diagonal as well by starting at i\n",
    "\n",
    "               values_subj.append(conn[i, j])  # Get the value for that specific pair (i, j)\n",
    "    \n",
    "        data_[dataset][s,:] = values_subj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06daae1-121e-4de0-8789-a94ee2869052",
   "metadata": {},
   "source": [
    "## Network SC to DF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35757001-f508-41cb-9a86-e0a8d1dc1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    colnames=[]\n",
    "    for i in range(len(df_network_names)):\n",
    "        for j in range(i, len(df_network_names)):\n",
    "\n",
    "            colnames.append(f'NSC_{df_network_names[i]}_{df_network_names[j]}')\n",
    "\n",
    "    df_ = pd.DataFrame(data_[dataset], columns=colnames)\n",
    "    \n",
    "    # Filter the main df_tabular by the current dataset\n",
    "    df_tabular_ = df_tabular.query(f\"dataset == '{dataset}'\")\n",
    "    \n",
    "    # Add the participant_ids from the subjects\n",
    "    #df_['participant_id'] = snakemake.params.subjects[dataset]\n",
    "    df_['participant_id'] = ['sub-'+subject for subject in subjects[dataset]]\n",
    "    \n",
    "    # Merge the new df_ with df_tabular_ on 'participant_id'\n",
    "    df_merged_ = pd.merge(df_tabular_, df_, how='left', on='participant_id')\n",
    "    \n",
    "    # Append merged DataFrame to the list\n",
    "    df_list.append(df_merged_)\n",
    "\n",
    "# Concatenate all merged DataFrames\n",
    "df_tabular = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf84477-3158-4561-9c32-2f8c492e2526",
   "metadata": {},
   "source": [
    "## Save regional and network SFC, SC & FC to masterlist ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4687a93-0fd0-482c-9493-9e919f77a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabular.to_csv(snakemake.output.tsv,sep='\\t',index=False)\n",
    "\n",
    "\n",
    "df_tabular "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
